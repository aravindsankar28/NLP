%\documentclass{report}
%\usepackage[utf8]{inputenc}
%\usepackage[margin=1in]{geometry}
%\usepackage[colorlinks]{hyperref}
%\renewcommand\thesection{\arabic{section}}
%\begin{document}
%\title{\textbf{Natural Language Processing - CS6370 }
%\\
%\textbf{Spell Check Assignment Report}}
%\author{Aravind Sankar CS11B033 \\
%Sriram V CS11B058 \\
%\\[0.2in]
%}

\documentclass[paper=a4, fontsize=10pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{verbatim,graphicx}
\usepackage{enumerate}
\usepackage[colorlinks]{hyperref}
\usepackage[margin=0.8in]{geometry}
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\normalfont\bf\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead[L]{\rule[-1.5ex]{0pt}{4ex}\small\scshape CS6370 -- Natural Language Processing} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyhead[R]{\small\scshape\thepage} % Page numbering for right footer
\fancyhfoffset[L]{2ex}
\fancyhfoffset[R]{2ex}
%\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\renewcommand\thesection{\arabic{section}}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Indian Institute of Technology Madras} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge CS6370 -- Natural Language Processing \\ Spell Check Assignment Report \\ % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}
\author{\large Aravind Sankar (CS11B033) \\ \large Sriram V (CS11B058)} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle
\thispagestyle{empty}
\section{Introduction}
This assignment involved designing an efficient spell checker. This spell checking assignment was divided into three parts: 
\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item Word checker where spelling corrections are given for a misspelled word not present in the dictionary.
\item Phrase checker where spelling corrections are given for misspelled word(s) present in phrases.
\item Sentence checker where spelling corrections are given misspelled word(s) present in sentences.
\end{itemize}

The Spell Checker was implemented in \textit{Python}.\\

The source code present in \texttt{/src} contains the following files and directories:
\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
    \item
        \texttt{word\_check.py} -- Performs spelling corrections on words.
    \item
        \texttt{phrase\_check.py} -- Performs spelling corrections on phrases and sentences. Uses the above file as a library.
    \item
        \texttt{metaphone.py} -- Contains the method to compute the double metaphone of a word. Imported into \texttt{word\_check.py}
    \item
        \texttt{data} -- This folder contains data that is used to build indices as a preprocessing step.
\end{itemize}

\section{Corpora Used}
\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt

\item The Unix dictionary for american english, which has close to 73,000 valid english words was used as the dictionary to identify if a given word has a spelling error.

\item Corpus of Contemporary American English (COCA) dataset (from \href{http://www.ngrams.info/download_coca.asp}{here}, which has 1,000,000 most frequent n-grams. This was used in the phrase and sentence spell checkers.	

\item Natural Language Corpus Data: Beautiful Data (from \href{http://norvig.com/ngrams/}{here}
) was used to obtain word and letter frequencies.

\item The brown corpus was also initially used for learning context words, but isn't part of the final spell checker model.

\end{itemize}

\section{Word Checker}
    The basic idea of approach for Word Checker was obtained from \cite{kern} which was titled - A Spelling Correction Program Based on a Noisy Channel Model. This paper doesn't take the context in which a misspelt word appears into account, and provides spelling corrections for standalone words.

Given a misspelt word $t$, we first find out a set of candidate corrections, and then find the  probability of a correction $c$ given $t$ using the Bayesian model as, 

\[Pr(c|t) = Pr(c) Pr(t|c)\]

where $Pr(c)$ represents the prior probability of the correction word $c$ estimated from a corpus , and $Pr(t|c)$ is the likelihood term which accounts for spelling transformations on the letters of the word. The spelling transformations include letter insertions, deletions, substitutions and transpositions.

$Pr(t|c)$ is estimated using the confusion matrices (\texttt{del[X,Y]}, \texttt{add[X,Y]}, \texttt{sub[X,Y]} and \texttt{rev[X,Y]}) given in \cite{kern}, which can be found in the \texttt{data} directory in the source folder as well. However, we found that the provided matrices were too sparse to be of much use at edit distances greater than $1$, and hence we augmented the ones provided in the paper by applying Laplace Smoothin (Add-One), to result in the 4 confusion matrices \texttt{addoneDelXY}, \texttt{addoneAddXY}, \texttt{addoneSubXY} and \texttt{addoneRevXY}.
The $\texttt{chars}[X,Y]$ matrix (not given in the paper) was estimated using letter bigram counts obtained from \href{http://norvig.com/ngrams/}{here}. This was appropriately scaled down using the ratio of sizes of the corpora to produce the final matrix \texttt{newCharsXY}. Further, the sum of the individual rows too was precomputed and saved to a fifth matrix \texttt{sumnewCharsXY}, in the interest of time.

The paper only considers correction words $c$ within an edit distance (the Damerau-Levenshtein distance, DL) of $1$ from the misspelt word $t$.
We extend this and consider correction words upto edit distance 3, beyond which we felt there would hardly be any corrections possible.

Instead of searching the entire set of words in the dictionary to find out words at edit distance $< 3$, we applied a few techniques for candidate selection and pruning the search space :

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item We create an inverted index of bigrams to set of words that contain the bigram. The set of words that contain the bigram, are also indexed by the length of the word. 
\item At query time, all possible bigrams of the misspelt word are found and we use the index structure to find all words that contain any of the bigrams, and also within length of $\pm$ 3 from the misspelt word. 
\item The set of candidate words obtained in the previous step are ranked according to the Jaccard Similarity index, and top-200 words are chosen for next stage.
\item We computed the Jaccard's Similarity for strings $x$ and $y$ by constructing the sets $X$ and $Y$ as the set of bigrams (letter) in strings $x$ and $y$ respectively. The jaccard similarity is computed as :

\[Sim(X,Y) = \frac{|X \cap Y|}{|X \cup Y|} \] 

\item Here, we observed that when jaccard's similarity was used, it fails to rank candidates properly when the misspelt word is small in length. This is because, if we have one spelling error, then 2 bigrams are lost, which leads a loss in rank and longer words get better tanks.
\item Hence we modified the similarity as :

\[Sim(X,Y) = \frac{|X \cap Y|}{|Y|} \] 
 where $Y$ is the set of bigrams of the correction (or candidate) word. This also gives more importance to the corrected word, unlike the Jaccard Similarity measure which is symmetric.

\item In this stage, we need to compute the likelihood scores for the candidates corrections obtained in the previous step. The dynamic programming solution for computing the DL distance has a complexity of $O(mn)$, where the length of the source and target words are $m$ and $n$.
\item We use the \texttt{trie} data structure to store these candidate words, and efficiently search the tree and return words within a given edit distance threshold ($3$ here).
\item The \texttt{trie} has a node for each letter of a word, and joins together nodes for words that share the same prefix. The dynamic programming solution to compute edit distance requires us to calculate the matrix row by row, for each letter in the source word. Thus, the \texttt{trie} gives us the advantage of having to compute each row only once, for all words that require this row.
\item This reduces the complexity of the computation to $O(m\textsl{<no. of nodes>})$, which is much, much lesser than the complexity of the brute force approach of computing edit distance for each of the words ($O(nm^2)$, where $m$ is the length of the longest word in the dictionary, and $n$ is the number of words in teh candidate set).

\item Storing the words in a \texttt{trie} helps prune out subtrees by using the edit distance as a threshold. The task of having to compute the likelihoods of each correction is also performed simultaneously, to speed up computation and reduce complexity.

\item In addition to using the prior probablity and likelihood to rank the words, we also calculate a phonetic score for each of the candidate words, which we multiply with the existing score.

\item The phonetic score is computed by evaluating the double metaphones of the misspelt word and each of the candidates, and assigning an appropriate score depending on whether they had a primary-primary match, primary-secondary match or no match.
\end{itemize}

Another approach using a BK tree was also tried to compute edit distance efficiently, but we found it to be inferior in comparison to our earlier approach, in terms of performance. Our distance function too inhibited our use of the BK tree, because we switched to a DL distance function that considers adjacent transpositions only, which is a non-metric function, whereas the BK tree relies on a metric function to prune efficiently.

\subsection{Likelihood Computation}
It turns out that unlike for the simple case of an edit distance of $1$ as handled in \cite{kern}, likelihood computation is a trivial task. In the case of greater edit distances, the problem becomes more involved since we need to break down the edits into a series of single edits, as we have confusion matrices for single edits only. Further, the order of these edits also comes into the picture. There is also the issue of multiple edit paths to convert a correct word into the misspelt word.\\

Hence, we could look at the problem in two ways -- a forward and a backward approach. The backward approach essentially involves traversing every edit path in reverse, multiplying single edit Naive Bayes probabilities along the way, as mentioned in the paper. Hence, this can be looked at as an error graph traversal of sorts and can be computed using a Depth-First Search (DFS) procedure. For this, at every node, we need to keep track of its parent nodes, and the edge labels (whether we arrived at that node via an insert, delete, subsitution or transposition), and accordingly move to the parent node. We initially implemented this, but the overhead involved made us switch over to the online, forward approach, which could be modelled as a dynamic programming problem similar to the edit distance one, and hence could be computed simultaneously along with the edit distance.\\

At an $i$, $j$ in the edit distance matrix, let the correct word seen till now be $s$, and the misspelt word be $tp$, where $t$ is a string and $p$ is the letter to be inserted. Then,
\[Pr(tp|s) = Pr(tp|sp)Pr(sp|s) + Pr(tp|t)Pr(t|s)\]
Here, the first term in the first product and the second one in the second product are single edits and hence, are computed at this step. $Pr(tp|sp) = Pr(t|s)$, and this value has been previously computed as part of the DP solution. Similarly, for deletes it is:
\[Pr(t|sp) = Pr(t|s)Pr(s|sp)+Pr(t|tp)Pr(tp|s)\]
The equation considered for substitutions works out to:
\[Pr(td|sp) = Pr(td|sd)Pr(sd|sp)+Pr(td|tp)Pr(tp|sp)\]
And for transpositions the equation considered is:
\[Pr(tjn|snj) = Pr(tjn|sjn)Pr(sjn|snj)+Pr(tjn|tnj)Pr(tnj|snj)\]

Here too, we see that the first term in the first product and the second one in the second product are both equal to $Pr(t|s)$ (which has already been computed), and the remaining products are all one edit distance away, and hence can be computed easily.\\

Thus, this algorithm was run simultaneously with the edit distance DP solution, to speed up the computation process. All the accessed entries were indexed to reduce the complexity of a single iteration to $O(1)$.

\section{Phrase and Sentence Checker}
We tried 2 different approaches for the phrase/sentence spell checking problem and finally chose one of them.
\subsection{Context words}
The first approach is based on the idea of context sensitive spelling correction, as is described in \cite{golding}. For the target word, they identify a confusion set, which is a set of possible words that could replace that word in a sentence. Since the paper deals with correcting spelling errors that result in valid words in the sentence, they assume that they have predefined confusion sets. In our case, since we're correcting misspelt words only, we use the top-20 candidates returned from our word checker as our confusion set. Let $w_i$ represent each word in the confusion set. The idea is that, each word $w_i$ in the confusion set will have a
characteristic distribution of words that occur in its context. Thus, to classify an ambiguous target word, we look at the words that occur around it and see which $w_i$'s distribution they closely follow. For this purpose, we look at the context words that occur in a window of $\pm k$ window of the target word. 

\[P(w_i|c_{-k},...,c_{1},...,c_{k}) = P(w_i) P(c_{-k},...,c_{1},...,c_{k})\]

\[P(w_i|c_{-k},...,c_{1},...,c_{k}) = \prod_{j \in -k,...,1,...,k} p(c_j|w_i) \]

We assume that the occurrence of each context word is independent of every other context word.

The values of $P(c_j|w_i)$ were estimated using the brown corpus.

When we tested it for the sample test cases, we didn't get very good results as the context words in phrases were too general to uniquely identify words in the confusion set based on frequency in the corpus.

\subsection{N-gram models}
\subsection*{Approach 1}
The next approach we tried was language models, more specifically n-gram models. A language model assigns a probability to a sentence, represented as a sequence of words. Suppose a sentence is represented as $W = w_1w_2w_3...w_n$, then we calculate $P(w)$ using the chair rule of probability as :
\[P(W) = P(w_1w_2w_3...w_n) = \prod_i P(w_i|w_1w_2...w_{i-1})\]

As estimating each of the joint probabilities from the corpus won't be feasible due to data sparsity, we looked at n-gram models, where we approximate the product as (for a k-gram model),
\[P(W) \approx \prod_i P(w_i|w_{i-k}...w_{i-1}) \]

We use the trigram and bigram frequency counts obtained from the COCA dataset.
As in the previous case, we obtain our confusion set from the output of our word checker. By substituting each word in the confusion set in place of the misspelt word in the phrase, we get a set of candidate phrases (or sentences). 
Using the trigram model, we find out the probability of each sentence, and rank them using these as scores.  This approach used the general idea of language models to do spelling correction for phrases, but it was found the following approach gave much better results for spelling correction.


\subsection*{Approach 2}
The idea behind our second approach based on language models was derived from \cite{bergsma}.
Out of the many techniques that paper describes, we adapt the \textbf{SUMLM} algorithm. 
We consider ngrams of sizes 5,4,3 and 2. Again, just like in the previous approach , we get a list of candidate sentences using the confusion set obtained from word checker.

Assuming the sentence has only one error, for a given sentence $w$, we find out possible n-grams which contain the target word, $\forall n \in \left\lbrace 2,3,4,5 \right\rbrace$. This gives rise to a maximum of 14 ngrams, which contain the target word. We find the ngram counts of each of these grams, and take the $log$ sum of each of these counts to get the score for that particular sentence.



Though this score gave correct phrases in the top-K results, it didn't rank them in the correct order. This was because we didn't take the likelihood factor (computed in word checker) into account. Without using likelihood to rank the scores, we had a few correction words that weren't so close to the misspelt word in terms of edit-distance and possible transformations (weren't that likely), but figured in the top of our list because they had larger ngram counts. To avoid this problem, we weighted our final score for each of the candidate words based on both the ngram model score and the likelihood score. This gave us the better results in comparison to the more traditional ngram model, as we include all possible ngram counts and hence cover even long range dependencies as much as possible.

This solved the case of one misspelt word per sentence/phrase. 

When we had more than one misspelt word, we followed a 2 step procedure :

\begin{itemize}
\itemsep1pt\parskip0pt\parsep0pt
\item First, we consider each misspelt word independent of the other one, and find a set of top-K (5 generally) corrections using the above procedure. This method has an obvious limitation that it ignores ngrams in which the other misspelt word(s) also occurs, as such an ngrams which contain the other misspelt word.
\item We take the top-K corrections for each of the misspelt words, and construction combinations or tuples. We then recompute the ngram model scores for each of these sentences, and rank these sentences to give our final set.
\end{itemize}

\section{Results}
\subsection{Word checker}
For the words given in \texttt{words.tsv}, our spell checker obtains an \textbf{MMR of 1} for all the words except \textit{failr} and \textit{thruout}.\\
In the case of the former, our model gives \textit{failure} as its first suggestion, which we believe is a better suggestion than the expected word \textit{fairy}, as phonetics is being taken into account. In the latter case, we obtain an \textbf{MMR of 0.5}, with \textit{throat} being the primary suggestion mainly because of the significant difference in edit distances ($3$ vs $1$ respectively). The word \textit{throughout} gets the second position, once again due to the influence of the phonetic score that has been computed using the double metaphones of the misspelt word and its suggestion.

\subsection{Phrase checker}
For the phrases specified in \texttt{phrases.tsv}, our spell checker once again obtains an \textbf{MMR of 1} for all the phrases specified, except for the cases where the errors are due to string concatenation (for example, \textit{halloffame}) or are grammatical in nature (such as the insertion of 'the' in \textit{president of united states}). The first issue can be tackled by making use of a word-splitting algorithm to test every word not present in the dictionary, before performing the actual spell check. The second case deals with grammar and syntax, and could thus be tackled by suggesting insertions based on either N-gram models or context windows.

\subsection{Sentence checker}
Finally, for the sentences mentioned in \texttt{sentences.tsv}, our model yet again gives an \textbf{MMR of 1} in all cases, if we choose to disregard the insertion of 'not' before 'to' in the misspelled sentence, \textit{To be or to bea is not the question.} This issue of insertion can be handled as mentioned above, for the case of phrases.

\begin{thebibliography}{9}
\bibitem{kern} Kernighan, M.D., Church, K.W., Gale, W.A.: A spelling correction program based on a noisy channel model. In: Proceedings of the 13th Conference on Computational Linguistics - Volume 2. pp. 205-210. COLING '90,
Association for Computational Linguistics, Stroudsburg, PA, USA (1990)
\bibitem{golding} Golding, A.R., Golding, A.I.: A bayesian hybrid method for context-sensitive
spelling correction. In: In Proceedings of the Third Workshop on Very Large
Corpora. pp. 39-53 (1995)
\bibitem{bergsma} Bergsma, S., Lin, D., Goebel, R.: Web-scale N-gram models for lexical disambiguation. In: Proceedings for the 21st International Joint Conference on Artificial Intelligence. pp. 1507-1512 (2009)
\end{thebibliography}

\end{document}
