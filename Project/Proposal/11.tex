%\documentclass{report}
%\usepackage[utf8]{inputenc}
%\usepackage[margin=1in]{geometry}
%\usepackage[colorlinks]{hyperref}
%\renewcommand\thesection{\arabic{section}}
%\begin{document}
%\title{\textbf{Natural Language Processing - CS6370 }
%\\
%\textbf{Spell Check Assignment Report}}
%\author{Aravind Sankar CS11B033 \\
%Sriram V CS11B058 \\
%\\[0.2in]
%}

\documentclass[paper=a4, fontsize=11pt]{scrartcl} % A4 paper and 11pt font size
\usepackage[T1]{fontenc} % Use 8-bit encoding that has 256 glyphs
\usepackage{fourier} % Use the Adobe Utopia font for the document - comment this line to return to the LaTeX default
\usepackage[english]{babel} % English language/hyphenation
\usepackage{amsmath,amsfonts,amsthm} % Math packages
\usepackage{verbatim,graphicx}
\usepackage{enumerate}
\usepackage[colorlinks]{hyperref}
\usepackage[margin=0.8in]{geometry}
\usepackage{sectsty} % Allows customizing section commands
\allsectionsfont{\normalfont\bf\scshape} % Make all sections centered, the default font and small caps

\usepackage{fancyhdr} % Custom headers and footers
\pagestyle{fancyplain} % Makes all pages in the document conform to the custom headers and footers
\fancyhead[L]{\rule[-1.5ex]{0pt}{4ex}\small\scshape CS6370 -- Natural Language Processing} % No page header - if you want one, create it in the same way as the footers below
\fancyfoot[L]{} % Empty left footer
\fancyfoot[C]{} % Empty center footer
\fancyhead[R]{\small\scshape\thepage} % Page numbering for right footer
\fancyhfoffset[L]{2ex}
\fancyhfoffset[R]{2ex}
%\renewcommand{\headrulewidth}{0pt} % Remove header underlines
\renewcommand{\footrulewidth}{0pt} % Remove footer underlines
\setlength{\headheight}{13.6pt} % Customize the height of the header
\renewcommand\thesection{\arabic{section}}

\numberwithin{equation}{section} % Number equations within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{figure}{section} % Number figures within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)
\numberwithin{table}{section} % Number tables within sections (i.e. 1.1, 1.2, 2.1, 2.2 instead of 1, 2, 3, 4)

%\setlength\parindent{0pt} % Removes all indentation from paragraphs - comment this line for an assignment with lots of text

%----------------------------------------------------------------------------------------
%	TITLE SECTION
%----------------------------------------------------------------------------------------

\newcommand{\horrule}[1]{\rule{\linewidth}{#1}} % Create horizontal rule command with 1 argument of height

\title{	
\normalfont \normalsize 
\textsc{Indian Institute of Technology Madras} \\ [25pt] % Your university, school and/or department name(s)
\horrule{0.5pt} \\[0.4cm] % Thin top horizontal rule
\huge CS6370 -- Natural Language Processing \\ Project Proposal \\ Email classification using Co-EM % The assignment title
\horrule{2pt} \\[0.5cm] % Thick bottom horizontal rule
}
\author{\large Aravind Sankar (CS11B033) \\ \large Sriram V (CS11B058)} % Your name

\date{\normalsize\today} % Today's date or a custom date

\begin{document}

\maketitle
\thispagestyle{empty}
\section{Motivation}
Our chosen topic to work on is Text Classification. This is a very broad area which has been well researched in the case of supervised learning of classifiers. We want to focus on classification using limited labeled data, i.e semi-supervised learning. The classification of email into different types is interesting problem where we face the problem of scarcity of labeled data. Of the many approaches to semi-supervised learning, we came across \cite{duck} which introduces the idea of Co-EM, which is hybrid method combining Co-Training and Expectation Maximization (EM). Existing techniques for Email Classification have tried EM and Co-Training algorithm separately as shown here \cite{dog}.  We feel that combining these techniques using Co-EM would yield better results for classification. To the best of our knowledge, Co-EM has not been used to solve email classification till now. The reason why we want to use Co-EM is  ---- . Our work will also involve trying out various classifiers and identifying the best suited one. This is a very essential task, as the learning problem has a very good chance of having a class imbalance.

\section{Datasets}


\section{Baseline Approaches}
The baselines with which we'd like to compare our approach would be the one described in \cite{duck} along with a few other approaches.  These are :

\begin{enumerate}
\item 
\end{enumerate}

\section{Evaluation Metrics}

\begin{thebibliography}{2}
\bibitem{duck} Analyzing the Effectiveness and Applicability
of Co-training. Kamal Nigam and Rayid Ghani - CIKM '00 Proceedings of the ninth international conference on Information and knowledge management. 

\bibitem{dog} Email classification with co-training. Svetlana Kiritchenko and Stan Matwin - 
CASCON '01 Proceedings of the 2001 conference of the Centre for Advanced Studies on Collaborative research
\end{thebibliography}

\end{document}
