Return-Path: ravi@cse.iitm.ac.in
Received: from mail.cse.iitm.ac.in (LHLO mail.cse.iitm.ac.in) (10.6.5.215)
 by mail.cse.iitm.ac.in with LMTP; Fri, 20 Sep 2013 13:56:26 +0530 (IST)
Received: from localhost (localhost.localdomain [127.0.0.1])
	by mail.cse.iitm.ac.in (Postfix) with ESMTP id 85515F781DA;
	Fri, 20 Sep 2013 13:55:12 +0530 (IST)
X-Virus-Scanned: amavisd-new at mail.cse.iitm.ac.in
X-Spam-Flag: NO
X-Spam-Score: 1.595
X-Spam-Level: *
X-Spam-Status: No, score=1.595 tagged_above=-10 required=6.6
	tests=[BAYES_50=0.8, HELO_NO_DOMAIN=0.001, HTML_MESSAGE=0.001,
	RDNS_NONE=0.793] autolearn=no
Received: from mail.cse.iitm.ac.in ([127.0.0.1])
	by localhost (mail.cse.iitm.ac.in [127.0.0.1]) (amavisd-new, port 10024)
	with ESMTP id RtbGrjLlT4EG; Fri, 20 Sep 2013 13:55:07 +0530 (IST)
Received: from mail.cse.iitm.ac.in (mail.cse.iitm.ac.in [10.6.5.215])
	by mail.cse.iitm.ac.in (Postfix) with ESMTP id 4FB9AF7812F;
	Fri, 20 Sep 2013 13:54:28 +0530 (IST)
Date: Fri, 20 Sep 2013 13:54:26 +0530 (IST)
From: "ravi@cse.iitm.ac.in" <ravi@cse.iitm.ac.in>
To: Seminar <seminar@cse.iitm.ac.in>
Cc: rise-iil <rise-iil@cse.iitm.ac.in>, aidb <aidb@googlegroups.com>
Message-ID: <1282512190.1017718.1379665466934.JavaMail.root@mail.cse.iitm.ac.in>
In-Reply-To: <1444517841.567850.1379424248800.JavaMail.root@mail.cse.iitm.ac.in>
Subject: Re: Machine Learning Seminar on FRIDAY, 20th.
MIME-Version: 1.0
Content-Type: multipart/alternative; 
	boundary="----=_Part_1017717_192247154.1379665466933"
X-Originating-IP: [10.65.0.32]
X-Mailer: Zimbra 6.0.7_GA_2473.DEBIAN5_64 (ZimbraWebClient - SAF3 (Mac)/6.0.7_GA_2473.DEBIAN5_64)

------=_Part_1017717_192247154.1379665466933
Content-Type: text/plain; charset=utf-8
Content-Transfer-Encoding: 7bit

Dear All, 


A gentle reminder about the talk this afternoon. 


Regards 
Ravi. 



Dear All, 


Dr. Gautam Kunapuli from Wisconsin-Madison will be giving a talk on Inverse Reinforcement Learning at 3 p.m. in BSB 361 on Friday, 20th. Details below. All are welcome. 


Regards 
Ravi. 





Title: Advice-Giving for Inverse Reinforcement Learning 


Abstract: 
Inverse Reinforcement Learning (IRL) is an approach for domain-reward discovery from demonstration, where an agent mines the reward function of a Markov decision process by observing an expert acting in the domain. In the standard setting, it is assumed that the expert acts (nearly) optimally, and a large number of trajectories, i.e., training examples are available for reward discovery (and consequently, learning domain behavior). These are not practical assumptions: trajectories are often noisy, and there can be a paucity of examples. This novel approach incorporates advice-giving into the IRL framework to address these issues. Inspired by preference elicitation, a domain expert provides advice on states and actions (features) by stating preferences over them. 


In this talk, I will provide a brief introduction to Inverse Reinforcement Learning and its applications. Then, I will present our advice-taking IRL approach, in which an agent learns from noisy demonstrations, and small amounts of targeted advice provided by a domain expert. Our results 


Bio: 
Gautam Kunapuli received his PhD in Applied Mathematics from Rensselaer Polytechnic Institute in 2008, under the guidance of Dr. Kristin Bennett. He was a Postdoctoral Research Associate at the University of Wisconsin-Madison from 2008-2013. He will be taking up a Research Scientist position at UtopiaCompression Corporation in October 2013. His research interests are primarily in developing efficient optimization approaches to diverse machine learning problems, including online learning, reinforcement learning and human-in-the-loop learning. 
------=_Part_1017717_192247154.1379665466933
Content-Type: text/html; charset=utf-8
Content-Transfer-Encoding: quoted-printable

<html><head><style type=3D'text/css'>p { margin: 0; }</style></head><body><=
div style=3D'font-family: Times New Roman; font-size: 12pt; color: #000000'=
>Dear All,<div><br></div><div>A gentle reminder about the talk this afterno=
on.</div><div><br></div><div>Regards</div><div>Ravi.<br><br><hr id=3D"zwchr=
"><style>p { margin: 0; }</style><div style=3D"font-family: Times New Roman=
; font-size: 12pt; color: #000000"><font face=3D"Times New Roman" size=3D"3=
">Dear All,</font><div style=3D"color: rgb(0, 0, 0); font-family: 'Times Ne=
w Roman'; font-size: 12pt;"><br></div><div style=3D"color: rgb(0, 0, 0); fo=
nt-family: 'Times New Roman'; font-size: 12pt;">Dr. Gautam Kunapuli from Wi=
sconsin-Madison will be giving a talk on Inverse Reinforcement Learning at =
3 p.m. in BSB 361 on Friday, 20th. Details below. All are welcome.</div><di=
v style=3D"color: rgb(0, 0, 0); font-family: 'Times New Roman'; font-size: =
12pt;"><br></div><div style=3D"color: rgb(0, 0, 0); font-family: 'Times New=
 Roman'; font-size: 12pt;">Regards</div><div style=3D"color: rgb(0, 0, 0); =
font-family: 'Times New Roman'; font-size: 12pt;">Ravi.</div><div style=3D"=
color: rgb(0, 0, 0); font-family: 'Times New Roman'; font-size: 12pt;"><br>=
</div><div><div><font face=3D"Times New Roman"><br></font></div><div><font =
face=3D"Times New Roman">Title: Advice-Giving for Inverse Reinforcement Lea=
rning</font></div><div><font face=3D"Times New Roman"><br></font></div><div=
><font face=3D"Times New Roman">Abstract:</font></div><div><font face=3D"Ti=
mes New Roman">Inverse Reinforcement Learning (IRL) is an approach for doma=
in-reward discovery from demonstration, where an agent mines the reward fun=
ction of a Markov decision process by observing an expert acting in the dom=
ain. In the standard setting, it is assumed that the expert acts (nearly) o=
ptimally, and a large number of trajectories, i.e., training examples are a=
vailable for reward discovery (and consequently, learning domain behavior).=
 These are not practical assumptions: trajectories are often noisy, and the=
re can be a paucity of examples. This novel approach incorporates advice-gi=
ving into the IRL framework to address these issues. Inspired by preference=
 elicitation, a domain expert provides advice on states and actions (featur=
es) by stating preferences over them.&nbsp;</font></div><div><font face=3D"=
Times New Roman"><br></font></div><div><font face=3D"Times New Roman">In th=
is talk, I will provide a brief introduction to Inverse Reinforcement Learn=
ing and its applications. Then, I will present our advice-taking IRL approa=
ch, in which an agent learns from noisy demonstrations, and small amounts o=
f targeted advice provided by a domain expert. Our results&nbsp;</font></di=
v><div><font face=3D"Times New Roman"><br></font></div><div><font face=3D"T=
imes New Roman">Bio:</font></div><div><font face=3D"Times New Roman">Gautam=
 Kunapuli received his PhD in Applied Mathematics from Rensselaer Polytechn=
ic Institute in 2008, under the guidance of Dr. Kristin Bennett. He was a P=
ostdoctoral Research Associate at the University of Wisconsin-Madison from =
2008-2013. He will be taking up a Research Scientist position at UtopiaComp=
ression Corporation in October 2013. His research interests are primarily i=
n developing efficient optimization approaches to diverse machine learning =
problems, including online learning, reinforcement learning and human-in-th=
e-loop learning.</font></div></div></div></div></div></body></html>
------=_Part_1017717_192247154.1379665466933--
