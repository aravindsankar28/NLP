Correction. It is on Wednesday. 


From: ssm@cse.iitm.ac.in 
To: "Seminar" <seminar@cse.iitm.ac.in> 
Cc: "ravi" <ravi@cse.iitm.ac.in>, sutanuc@cse.iitm.ac.in, elango@cse.iitm.ac.in, sandipan@iitm.ac.in, "mani10j" <mani10j@gmail.com> 
Sent: Tuesday, June 25, 2013 3:06:36 PM 
Subject: MS Seminar 






Dear All, 


I will be giving my MS seminar talk on 26th June , 2013 (Wednesday ) at 2:00 PM. I request you to kindly attend the seminar and give your valuable suggestions. The details of the talk are as follows: 



Speaker : Manimaran S S 
Date : 26-06-2013, Wednesday 
Time : 14:00 hrs to 15:00 hrs 
Topic : Policy Iteration on Continuous Domains using Rapidly-Exploring Random Trees 
Venue : BSB 361, Department of Computer Science and Engineering. 




Abstract 



Reinforcement Learning(RL) deals with learning optimal control strategies through experience or interactions with the world. One of the major hindrances in extending existing RL techniques to real world scenarios, is their inability to deal with continuous valued variables. Techniques such as variable discretization and using function approximators in place of lookup tables in the original discrete algorithms exist in an offline setting. However, the question of how exactly these interactions should be carried out exists. Attempts have been made to address this problem of online exploration in large and continuous domains. 

In this seminar we present an algorithm - RRTPI that combines ideas from the domain of continuous path planning to clearly provide a method to generate sufficiently representative samples and ensure exploration. Rapidly-exploring random trees(RRTs) are a class of sampling based planning methods that have been used to solve for feasible trajectories from a given starting to goal state in kinodynamic spaces. They can be shown to possess useful properties such as asymptotic completeness and asymptotic optimality. RRTPI is an iterative algorithm that generates sample trajectories based on a value function and then improves the estimate of the value function using the newly generated samples. RRTPI is a sound algorithm that gives specific ways to generate samples such that it has guaranteed exploration properties. The entire algorithm is free from learning rates and exploration control parameters. 

Underactuated and underpowered domains pose a problem for RRTs as the exploration guarantees do not hold. This is because the distance metric used in the algorithms is usually Euclidean distance. Whereas in order to possess good behavior a good distance metric must be `learnt'. We show that RRTPI does this by using the value function as a distance metric. Compared to existing techniques for handling underactuation, our method is more generalized and makes lesser assumptions. 






All are welcome! 




Regards, 

Manimaran S S, 

CS1 0S026 , 

MS Scholar, 

RISE Lab, 

CSE Department, 

IIT Madras. 

