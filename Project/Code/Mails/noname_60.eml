


A gentle remainder !!=20


Dear All,=20


I invite you to my MS seminar on 15th April 2014 (Tuesday) at 4:00 PM. Kind=
ly make it convenient to attend the seminar and give your valuable feedback=
.=20


The details of the seminar are as follows.=20


Title : Learning Compact Yet Rich Models for Text=20
Speaker : Sarath Chandar A P (CS12S043)=20
Time : 15-04-2014 (Tuesday) at 4:00 PM=20
Venue : BSB 361, Seminar Hall, Department of Computer Science and Engineeri=
ng=20


Abstract :=20
Processing any Natural Language requires the understanding of both syntax a=
nd semantics of that language. Several learning algorithms were proposed to=
 learn syntax and semantics from labeled text data. It is observed that as =
the models become richer, the process of learning becomes intractable due t=
o increase in the complexity of the models. In this work, we propose method=
s to learn rich models that tackle the Complexity versus Learnability trade=
 o=EF=AC=80. The models proposed are rich, compact and also easily learnabl=
e.=20





The =EF=AC=81rst part of the talk focuses on learning grammar from labeled =
text data. Learning rich grammar is challenging but yet has huge impact on =
several diverse applications like Machine Translation, Information Extracti=
on and Sentence Compression. Most of the work in the literature focuses on =
learning Context Free Grammars (CFGs). On the other hand, Context Sensitive=
 Grammars (CSGs) are more expressible but di=EF=AC=83cult to learn. It is o=
bserved that there exists a class of grammars called Mildly Context Sensiti=
ve Grammars (MCSGs) like Tree Adjoining Grammars (TAGs), which is more expr=
essible than CFGs and easy to learn when compared to CSGs. In this work, we=
 propose a Bayesian non-parametric model to learn compact rules for TAGs. T=
he parser incorporating the learnt rules performs better than other parsers=
 when tested on Wall Street Journal section of Penn Tree bank.=20

Languages such as English have plethora of training data available unlike m=
any other languages which are increasingly prevalent in the current Interne=
t. We try to address the problem of learning a uni=EF=AC=81ed model to repr=
esent semantics of multiple languages. This uni=EF=AC=81ed model will facil=
itate in training classi=EF=AC=81ers using data available across languages.=
 In the second part of the talk, we then propose a cross language learning =
technique based on auto-encoders to learn common representation for two di=
=EF=AC=80erent languages. To the best of our knowledge, this is =EF=AC=81rs=
t attempt at representing bilingual words without using word alignments acr=
oss the language pair. The proposed model is both task and language indepen=
dent. The performance of the model is veri=EF=AC=81ed by testing on three d=
i=EF=AC=80erent tasks each with three different language pairs. In Cross La=
nguage Document Classification, the proposed model performs 14% better than=
 the current state-of-the-art.=20


All are welcome !=20


Regards,=20
Sarath Chandar A P,=20
RISE Lab.=20


