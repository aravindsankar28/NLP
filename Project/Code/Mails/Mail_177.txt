Hi All

I will be presenting briefly on my dual degree project work.

It is hardly going to be any technical talk. So, everyone is free to join.

Where: Meeting Room 1
When: 16th June (Today! Monday)

You can find a demonstration of my work using a *quadrotor mounted with a
monocular camera* on youtube.
https://www.youtube.com/watch?v=3DRyAtT7UOBpQ

Thank you,

Keywords: Quadrotor, Human Robot Interaction, User Interface.

*Abstract:*
*Every day, robots are becoming smarter and faster, and will soon be quite*
*ubiquitous. But they are still yet unable to fully understand human
intents*
*as well humans can understand other humans. Similarly, there are tasks
like*
*scene understanding that are immensely difficult, if not impossible, for
robots*
*at present. On the other hand, humans can perform the same tasks quite
easily.*

*In our work, we have identified two gaps for a well-functioning human-*
*robot system, viz. in communication, and in perception. We cannot expect a=
*
*human user to tediously control a robot using manual controls. Thus, it is
im-*
*portant for the user to be able to communicate her intent clearly and
unambigu-*
*ously, but still easily. But human communication is usually ambiguous and
can*
*be resolved only by knowing the context, like their current surroundings.
In*
*such cases, while other humans can easily easily disambiguate since they
share*
*the context by understanding the scene around them effortlessly, robot
percep-*
*tion is not there yet to be able to understand ambigous human intents.
There-*
*fore, in our work, we tackle this problem by empirically trying some
methods*
*that couple together perception and communication of human and robot in an=
*
*attempt to find the right balance.*

*In this work, we have implemented a working system with which a user can*
*intuitively control a commercial quadrotor through our semantic interface.
This*
*is enabled by making the quadrotor minimally understand the scene and use*
*this to discern the user=E2=80=99s intent unambiguously. We have also done=
 the
reverse*
*process where the user=E2=80=99s gesture helps the quadrotor in its percep=
tion
process*
*to understand the scene. Demonstrations were first run using a simulator
and*
*later were followed by demonstrations with an actual quadrotor in an
unknown*
*real world scene.*

--=20
Devesh Yamparala
Final Yr, Undergrad
Dept. of Computer Science and Engineering
Indian Institute of Technology Madras
